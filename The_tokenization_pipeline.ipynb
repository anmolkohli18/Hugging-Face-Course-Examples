{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2iit4mcv/myg4Cwb3LEH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmolkohli18/Hugging-Face-Course-Examples/blob/main/The_tokenization_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sbz9WQwqXZzF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert Tokenization"
      ],
      "metadata": {
        "id": "SbsRGJXEat2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "IoRH2fspaX-B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(\"Let's try to tokenize and call it tokenization!\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_6Bme0raaSl",
        "outputId": "d25cf8b8-b4f9-4b58-cb59-a9ca6e9ff6ac"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['let', \"'\", 's', 'try', 'to', 'token', '##ize', 'and', 'call', 'it', 'token', '##ization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVryC-JxbG-m",
        "outputId": "21498c9e-c75c-4691-f8bd-2304d3ef6dcf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2292, 1005, 1055, 3046, 2000, 19204, 4697, 1998, 2655, 2009, 19204, 3989, 999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
        "print(final_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_2JXoTvbfQb",
        "outputId": "fe94bf91-24c9-46e7-e61a-208b66ea3a23"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 1998, 2655, 2009, 19204, 3989, 999, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Let's try to tokenize and call it tokenization!\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpKp0L4pb5u_",
        "outputId": "a5e25fd5-4bbb-4e1c-8e77-63f2153a839e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] let's try to tokenize and call it tokenization! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKyW8EoFdOQW",
        "outputId": "d045d0d0-37fe-45b8-954e-1e8285c9ba26"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2292, 1005, 1055, 3046, 2000, 19204, 4697, 1998, 2655, 2009, 19204, 3989, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Albert Tokenization"
      ],
      "metadata": {
        "id": "EqdfbQElaxFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "DTG6qDaualTD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
        "tokens = tokenizer.tokenize(\"Let's try to tokenize and call it tokenization!\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8fQhSkna1FB",
        "outputId": "b356c739-a2e4-45ed-db0e-e7974699bfb8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁let', \"'\", 's', '▁try', '▁to', '▁to', 'ken', 'ize', '▁and', '▁call', '▁it', '▁to', 'ken', 'ization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLE9iy3ba_HQ",
        "outputId": "5c44012d-4e33-416e-9faf-808d336b8a45"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[408, 22, 18, 1131, 20, 20, 2853, 2952, 17, 645, 32, 20, 2853, 1829, 187]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
        "print(final_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J-EJYL8bSe8",
        "outputId": "806dce73-f315-489c-fdb3-30bdb2d37a55"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 408, 22, 18, 1131, 20, 20, 2853, 2952, 17, 645, 32, 20, 2853, 1829, 187, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Let's try to tokenize and call it tokenization!\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-pfhpZYbtJB",
        "outputId": "9422aa2a-ebde-4d7f-ff4d-38775d62a41f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] let's try to tokenize and call it tokenization![SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Roberta Tokenizer"
      ],
      "metadata": {
        "id": "5SYaXZ73cVEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "lSnC1tnqcQnE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "tokens = tokenizer.tokenize(\"Let's try to tokenize and call it tokenization!\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqvYThU_cZuD",
        "outputId": "723c1244-ab19-4bda-f180-297fa3aa24c3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'s\", 'Ġtry', 'Ġto', 'Ġtoken', 'ize', 'Ġand', 'Ġcall', 'Ġit', 'Ġtoken', 'ization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohnkb-rEckmj",
        "outputId": "9d2f3e09-43a4-4b8b-84df-40e37d7d7c22"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7939, 18, 860, 7, 19233, 2072, 8, 486, 24, 19233, 1938, 328]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
        "print(final_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIZlsbG2cpzR",
        "outputId": "b278b32c-fc3c-4b6b-b6bb-c4e586f9feeb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 7939, 18, 860, 7, 19233, 2072, 8, 486, 24, 19233, 1938, 328, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Let's try to tokenize and call it tokenization!\")\n",
        "print(tokenizer.decode(inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEmlw8facxtR",
        "outputId": "de082497-3845-4721-9917-d8c9aab4e2c5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>Let's try to tokenize and call it tokenization!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YMAQlQxgc5sA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}